# Fine-tuning Your Google Reviews Classifier

## ğŸ¯ Overview

You now have **1,164 labeled Google Reviews** that you can use to fine-tune custom models! Here are your options:

## ğŸ“Š Your Dataset Summary
- **Total Reviews**: 1,164
- **Labels**: 7 categories (genuine_positive, genuine_negative, spam, advertisement, irrelevant, fake_rant, inappropriate)
- **Quality**: Multi-LLM labeled with confidence scores
- **File**: `google_reviews_labeled_combined.csv`

## ğŸš€ Fine-tuning Options

### Option 1: Hugging Face Transformers (Recommended)
**Best for**: Production deployment, custom control, no API costs

**Models to try**:
- `distilbert-base-uncased` - Fast and efficient
- `roberta-base` - Better performance
- `bert-base-uncased` - Classic choice

**Steps**:
1. Install dependencies: `pip install transformers datasets torch sklearn`
2. Run: `python finetune_review_classifier.py`
3. Follow interactive prompts

**Advantages**:
- âœ… No API costs
- âœ… Full control over training
- âœ… Can deploy anywhere
- âœ… Fast inference

### Option 2: OpenAI Fine-tuning
**Best for**: Highest quality, complex reasoning

**Steps**:
1. Install: `pip install openai`
2. Prepare data: `python openai_finetune_prep.py`
3. Run fine-tuning script
4. Estimated cost: ~$25-50 for your dataset

**Advantages**:
- âœ… Highest quality results
- âœ… Advanced reasoning capabilities
- âœ… Easy to use

### Option 3: Quick Local Training (Scikit-learn)
**Best for**: Quick prototypes, baseline models

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load data
df = pd.read_csv('google_reviews_labeled_combined.csv')
X = df['text']
y = df['llm_classification']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000, stop_words='english')),
    ('classifier', LogisticRegression(max_iter=1000))
])

# Train
pipeline.fit(X_train, y_train)

# Evaluate
y_pred = pipeline.predict(X_test)
print(classification_report(y_test, y_pred))

# Save model
import joblib
joblib.dump(pipeline, 'review_classifier_sklearn.pkl')
```

## ğŸ”„ Recommended Workflow

1. **Start with Option 3** (Scikit-learn) for quick baseline
2. **Try Option 1** (Transformers) for production quality
3. **Consider Option 2** (OpenAI) if you need highest accuracy

## ğŸ“ˆ Expected Performance

Based on your dataset quality and size:

- **Scikit-learn**: ~70-80% accuracy
- **Fine-tuned BERT**: ~85-90% accuracy  
- **Fine-tuned GPT**: ~90-95% accuracy

## ğŸ›  Next Steps

1. **Choose your approach** based on needs/budget
2. **Install required dependencies**
3. **Run the training scripts**
4. **Evaluate and iterate**

## ğŸ’¡ Pro Tips

- **Start small**: Train on 500 reviews first to test
- **Monitor overfitting**: Use validation sets
- **Ensemble methods**: Combine multiple models
- **Active learning**: Add more labeled data where models disagree

## ğŸ” Model Evaluation

After training, compare your custom model with:
- Your original LLM labels
- BART classifier results  
- Human evaluation on sample

This will give you confidence in your model's performance!

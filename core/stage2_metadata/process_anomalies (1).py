"""
Process Anomaly Reports for Training Data

This script processes the anomaly report generated by metadata_analyzer.py
and creates/updates a training dataset with labeled suspicious and edge cases.
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict
import logging
import random

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AnomalyProcessor:
    """Process anomaly reports and update training data."""
    
    def __init__(self, report_path: str, features_csv: str = None, output_dir: str = "output"):
        """Initialize the processor with report path, optional features CSV, and output directory."""
        self.report_path = Path(report_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.training_data_path = self.output_dir / "anomaly_training_data.csv"
        self.features_csv = Path(features_csv) if features_csv else None
        self.report = None
        self.features_df = None

        if self.features_csv and self.features_csv.exists():
            try:
                self.features_df = pd.read_csv(self.features_csv, low_memory=False)
                logger.info(
                    f"âœ… Loaded features CSV with {len(self.features_df)} rows, "
                    f"columns: {self.features_df.columns.tolist()}"
                )

                # ðŸ”„ Normalize feature column names
                rename_map = {}
                if "reviewId" in self.features_df.columns:
                    rename_map["reviewId"] = "review_id"
                if "reviewerId" in self.features_df.columns:
                    rename_map["reviewerId"] = "reviewer_id"
                if "placeId" in self.features_df.columns:
                    rename_map["placeId"] = "place_id"

                if rename_map:
                    self.features_df.rename(columns=rename_map, inplace=True)
                    logger.info(f"ðŸ”„ Renamed columns: {rename_map}")

            except Exception as e:
                logger.warning(f"Failed to load features CSV: {e}")


    def load_report(self) -> Dict:
        """Load the anomaly report JSON."""
        try:
            logger.info(f"Looking for report at: {self.report_path.absolute()}")
            if not self.report_path.exists():
                raise FileNotFoundError(f"Report file not found at {self.report_path.absolute()}")
                
            with open(self.report_path, 'r') as f:
                self.report = json.load(f)
                
            if not isinstance(self.report, dict):
                raise ValueError(f"Invalid report format. Expected dict, got {type(self.report)}")
                
            logger.info(f"Successfully loaded report with keys: {list(self.report.keys())}")
            return self.report
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {str(e)}")
            raise
        except Exception as e:
            logger.error(f"Error loading report: {str(e)}")
            raise

    def process_anomalies(self) -> pd.DataFrame:
        """Process anomalies into labeled training data."""
        if not self.report:
            self.load_report()
            
        records = []
        normal_records = []
        
        # Collect all known review IDs from features CSV if available
        all_reviews = set()
        if self.features_df is not None and "review_id" in self.features_df.columns:
            all_reviews.update(self.features_df["review_id"].astype(str).tolist())
        
        # Track anomalous review IDs to avoid duplicates
        anomalous_review_ids = set()
        
        # --- Rule-based anomalies ---
        top_anomalies = self.report.get("top_anomalies", {})
        logger.info(f"Found {len(top_anomalies)} rule-based anomalies")
        
        for feature, details in top_anomalies.items():
            if feature in ["ml_anomaly", "ml_anomaly_score"]:
                continue
                
            review_id = str(details.get("review_id") or details.get("reviewId"))
            if not review_id:
                continue
                
            anomalous_review_ids.add(review_id)
            
            records.append({
                "review_id": review_id,
                "reviewer_id": details.get("reviewer_id") or details.get("reviewerId", ""),
                "place_id": details.get("place_id") or details.get("placeId", ""),
                "feature": feature,
                "score": details.get("max_score", 0),
                "source": "rule_based",
                "label": 1
            })
        
        # --- ML-based anomalies ---
        ml_summary = self.report.get("ml_anomaly_summary", {})
        ml_anomalies = ml_summary.get("top_anomalies", [])
        logger.info(f"Found {len(ml_anomalies)} ML-based anomalies")
        
        for item in ml_anomalies:
            review_id = str(item.get("reviewId") or item.get("review_id"))
            if not review_id:
                continue
                
            anomalous_review_ids.add(review_id)
            score = item.get("ml_anomaly_score", 0)
            
            reviewer_id = item.get("reviewerId") or item.get("reviewer_id", "")
            place_id = item.get("placeId") or item.get("place_id", "")
            if self.features_df is not None:
                row = self.features_df[self.features_df["review_id"].astype(str) == review_id]
                if not row.empty:
                    reviewer_id = reviewer_id or row.iloc[0].get("reviewer_id", "")
                    place_id = place_id or row.iloc[0].get("place_id", "")
            
            records.append({
                "review_id": review_id,
                "reviewer_id": reviewer_id,
                "place_id": place_id,
                "feature": "ml_anomaly",
                "score": score,
                "source": "ml",
                "label": 1
            })
        
        # --- Add normal reviews (non-anomalous) ---
        normal_review_ids = list(all_reviews - anomalous_review_ids)
        num_normal = min(len(anomalous_review_ids) * 2, len(normal_review_ids))  # Up to 2x normal samples
        
        if num_normal > 0:
            normal_review_ids = random.sample(normal_review_ids, num_normal)
            logger.info(f"Adding {len(normal_review_ids)} normal reviews for balance")
            
            for review_id in normal_review_ids:
                reviewer_id, place_id = "", ""
                if self.features_df is not None:
                    row = self.features_df[self.features_df["review_id"].astype(str) == review_id]
                    if not row.empty:
                        reviewer_id = row.iloc[0].get("reviewer_id", "")
                        place_id = row.iloc[0].get("place_id", "")
                
                normal_records.append({
                    "review_id": review_id,
                    "reviewer_id": reviewer_id,
                    "place_id": place_id,
                    "feature": "normal",
                    "score": 0,
                    "source": "normal",
                    "label": 0
                })
        
        # Combine anomalous and normal records
        df = pd.DataFrame(records + normal_records)
        logger.info(f"Processed {len(df)} total records ({len(records)} anomalous, {len(normal_records)} normal)")
        
        # Add top features if available
        ml_results = self.report.get("ml_results", {})
        top_features = ml_results.get("top_features", {})
        for feature in top_features.keys():
            if feature not in df.columns and feature != "label":
                df[feature] = 0  # placeholder column for model training
        
        return df

    def update_training_data(self, new_records: pd.DataFrame) -> pd.DataFrame:
        """Update training data with new records, avoiding duplicates."""
        try:
            existing = pd.read_csv(self.training_data_path)
            existing = existing[~existing['review_id'].isin(new_records['review_id'])]
            combined = pd.concat([existing, new_records], ignore_index=True)
        except FileNotFoundError:
            combined = new_records
            
        combined.to_csv(self.training_data_path, index=False)
        logger.info(f"Updated training data saved to {self.training_data_path}")
        return combined
    
    def generate_summary(self, df: pd.DataFrame) -> Dict:
        """Generate a summary of the processed anomalies."""
        if df.empty:
            return {"total_anomalies": 0}
            
        return {
            "total_records": len(df),
            "by_label": df['label'].value_counts().to_dict(),
            "by_source": df['source'].value_counts().to_dict(),
            "top_features": df[df['feature'] != 'ml_anomaly']['feature'].value_counts().head(5).to_dict()
        }
    
    def run(self) -> Dict:
        """Run the complete processing pipeline."""
        try:
            logger.info("Starting anomaly processing...")
            self.load_report()
            
            logger.info("Processing anomalies...")
            anomalies_df = self.process_anomalies()
            
            if anomalies_df.empty:
                logger.warning("No anomalies found to process")
                return {
                    "status": "success",
                    "records_processed": 0,
                    "summary": {"total_anomalies": 0},
                    "training_data_path": str(self.training_data_path)
                }
            
            logger.info("Updating training data...")
            training_df = self.update_training_data(anomalies_df)
            summary = self.generate_summary(training_df)
            
            logger.info(f"Processing complete. Found {len(anomalies_df)} new anomalies.")
            logger.info(f"Label distribution: {summary.get('by_label', {})}")
            
            return {
                "status": "success",
                "records_processed": len(anomalies_df),
                "summary": summary,
                "training_data_path": str(self.training_data_path)
            }
            
        except Exception as e:
            logger.error(f"Error processing anomalies: {str(e)}")
            return {
                "status": "error",
                "error": str(e)
            }

def main():
    """Main entry point for the script."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Process anomaly reports into training data.')
    default_report = str(Path(__file__).parent / 'output' / 'anomaly_report.json')
    default_features = str(Path(__file__).parent / 'output' / 'metadata_features.csv')
    default_output = str(Path(__file__).parent / 'output')

    
    parser.add_argument('--report', type=str, default=default_report,
                      help='stage_2_metadata_anomaly')
    parser.add_argument('--features', type=str, default=default_features,
                      help='stage_2_metadata_anomaly')
    parser.add_argument('--output-dir', type=str, default=default_output,
                        help='stage_2_metadata_anomaly')

    
    args = parser.parse_args()
    
    processor = AnomalyProcessor(args.report, args.features, args.output_dir)
    result = processor.run()
    
    if result['status'] == 'success':
        print("\n=== Processing Complete ===")
        print(f"Total records processed: {result['records_processed']}")
        print(f"Training data saved to: {result['training_data_path']}")
        print("\nSummary:")
        for k, v in result['summary'].items():
            print(f"- {k}: {v}")
    else:
        print(f"Error: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()
